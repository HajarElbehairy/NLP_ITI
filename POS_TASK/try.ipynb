{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b7934a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Collecting pyconll\n",
      "  Downloading pyconll-3.2.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Downloading pyconll-3.2.0-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: pyconll\n",
      "Successfully installed pyconll-3.2.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~%p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~-p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~0p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~1p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~2p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~3p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~4p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~5p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~6p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~7p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~8p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~=p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~~p (c:\\Python311\\Lib\\site-packages)\n",
      "DEPRECATION: Loading egg at c:\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "WARNING: Ignoring invalid distribution ~%p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~-p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~0p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~1p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~2p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~3p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~4p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~5p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~6p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~7p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~8p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~=p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~~p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~%p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~-p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~0p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~1p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~2p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~3p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~4p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~5p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~6p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~7p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~8p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~=p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~~p (c:\\Python311\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pyconll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af6cc570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting camel-tools\n",
      "  Downloading camel_tools-1.5.6-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting future (from camel-tools)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: six in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from camel-tools) (1.17.0)\n",
      "Collecting docopt (from camel-tools)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: cachetools in c:\\python311\\lib\\site-packages (from camel-tools) (5.3.2)\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from camel-tools) (1.24.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from camel-tools) (1.10.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from camel-tools) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from camel-tools) (1.2.2)\n",
      "Collecting dill (from camel-tools)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch>=2.0 in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from camel-tools) (2.1.1)\n",
      "Collecting transformers<4.44.0,>=4.0 (from camel-tools)\n",
      "  Downloading transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting editdistance (from camel-tools)\n",
      "  Downloading editdistance-0.8.1-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from camel-tools) (2.31.0)\n",
      "Requirement already satisfied: emoji in c:\\python311\\lib\\site-packages (from camel-tools) (2.14.1)\n",
      "Collecting pyrsistent (from camel-tools)\n",
      "  Downloading pyrsistent-0.20.0-cp311-cp311-win_amd64.whl.metadata (976 bytes)\n",
      "Collecting tabulate (from camel-tools)\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: tqdm in c:\\python311\\lib\\site-packages (from camel-tools) (4.66.1)\n",
      "Collecting muddler (from camel-tools)\n",
      "  Downloading muddler-0.1.3-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from torch>=2.0->camel-tools) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\python311\\lib\\site-packages (from torch>=2.0->camel-tools) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\python311\\lib\\site-packages (from torch>=2.0->camel-tools) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\python311\\lib\\site-packages (from torch>=2.0->camel-tools) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from torch>=2.0->camel-tools) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\python311\\lib\\site-packages (from torch>=2.0->camel-tools) (2023.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from transformers<4.44.0,>=4.0->camel-tools) (0.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python311\\lib\\site-packages (from transformers<4.44.0,>=4.0->camel-tools) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python311\\lib\\site-packages (from transformers<4.44.0,>=4.0->camel-tools) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python311\\lib\\site-packages (from transformers<4.44.0,>=4.0->camel-tools) (2023.12.25)\n",
      "Collecting safetensors>=0.4.1 (from transformers<4.44.0,>=4.0->camel-tools)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<4.44.0,>=4.0->camel-tools)\n",
      "  Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from tqdm->camel-tools) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from pandas->camel-tools) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from pandas->camel-tools) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from requests->camel-tools) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests->camel-tools) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests->camel-tools) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python311\\lib\\site-packages (from requests->camel-tools) (2023.5.7)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn->camel-tools) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from scikit-learn->camel-tools) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python311\\lib\\site-packages (from jinja2->torch>=2.0->camel-tools) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\python311\\lib\\site-packages (from sympy->torch>=2.0->camel-tools) (1.3.0)\n",
      "Downloading camel_tools-1.5.6-py3-none-any.whl (124 kB)\n",
      "Downloading transformers-4.43.4-py3-none-any.whl (9.4 MB)\n",
      "   ---------------------------------------- 0.0/9.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/9.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.8/9.4 MB 2.1 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.0/9.4 MB 2.1 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.3/9.4 MB 2.1 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.1/9.4 MB 2.1 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 2.4/9.4 MB 2.1 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.9/9.4 MB 2.1 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 3.4/9.4 MB 2.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.9/9.4 MB 2.1 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 4.2/9.4 MB 2.1 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 4.7/9.4 MB 2.1 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 5.0/9.4 MB 2.1 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 5.5/9.4 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.0/9.4 MB 2.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.6/9.4 MB 2.1 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 6.8/9.4 MB 2.1 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 7.3/9.4 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.3/9.4 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.1/9.4 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.7/9.4 MB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.2/9.4 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.4/9.4 MB 2.1 MB/s eta 0:00:00\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading editdistance-0.8.1-cp311-cp311-win_amd64.whl (79 kB)\n",
      "Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Downloading muddler-0.1.3-py3-none-any.whl (16 kB)\n",
      "Downloading pyrsistent-0.20.0-cp311-cp311-win_amd64.whl (63 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Downloading tokenizers-0.19.1-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.3/2.2 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.8/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.2 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.1/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 2.1 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13773 sha256=cbb77a30cce995deb3c0776a3d3d70e7e03c55eb85d71e8b9ea35732c4fc389e\n",
      "  Stored in directory: c:\\users\\dell\\appdata\\local\\pip\\cache\\wheels\\1a\\b0\\8c\\4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
      "Successfully built docopt\n",
      "Installing collected packages: docopt, tabulate, safetensors, pyrsistent, muddler, future, editdistance, dill, tokenizers, transformers, camel-tools\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~%p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~-p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~0p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~1p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~2p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~3p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~4p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~5p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~6p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~7p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~8p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~=p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~~p (c:\\Python311\\Lib\\site-packages)\n",
      "DEPRECATION: Loading egg at c:\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "WARNING: Ignoring invalid distribution ~%p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~-p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~0p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~1p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~2p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~3p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~4p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~5p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~6p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~7p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~8p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~=p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~~p (c:\\Python311\\Lib\\site-packages)\n",
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'c:\\\\Python311\\\\Scripts\\\\tabulate.exe' -> 'c:\\\\Python311\\\\Scripts\\\\tabulate.exe.deleteme'\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install camel-tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33c217",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'camel_tools.lemmatizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcamel_tools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlemmatizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdefault\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultLemmatizer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcamel_tools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mword\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m simple_word_tokenize\n\u001b[0;32m      4\u001b[0m lemmatizer \u001b[38;5;241m=\u001b[39m DefaultLemmatizer()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'camel_tools.lemmatizer'"
     ]
    }
   ],
   "source": [
    "from camel_tools import lim\n",
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "\n",
    "lemmatizer = DefaultLemmatizer()\n",
    "text = 'وكتبتها'\n",
    "tokens = simple_word_tokenize(text)\n",
    "\n",
    "lemmas = [lemmatizer.lemmatize(token)[0] for token in tokens]\n",
    "print(lemmas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0a4dad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyconll\n",
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "747ed6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review the following sentences:\n",
      "afp.20000815.0007:p3u1\n",
      "afp.20000815.0017:p10u1\n",
      "afp.20000815.0020:p4u1\n",
      "afp.20000815.0020:p4u1\n",
      "afp.20000815.0020:p9u1\n",
      "afp.20000815.0022:p8u1\n",
      "afp.20000815.0026:p5u1\n",
      "afp.20000815.0028:p3u1\n",
      "afp.20000815.0040:p4u1\n",
      "afp.20000815.0042:p8u1\n",
      "afp.20000815.0042:p14u1\n",
      "afp.20000815.0044:p6u1\n",
      "afp.20000815.0044:p6u1\n",
      "afp.20000815.0056:p4u1\n",
      "afp.20000815.0072:p24u1\n",
      "afp.20000815.0089:p3u1\n",
      "afp.20000815.0107:p2u1\n",
      "afp.20000815.0113:p4u1\n",
      "afp.20000815.0113:p4u1\n",
      "afp.20000815.0122:p5u1\n",
      "afp.20000815.0129:p5u1\n",
      "afp.20000815.0143:p5u1\n",
      "afp.20000915.0008:p2u1\n",
      "afp.20000915.0008:p3u1\n",
      "afp.20000915.0011:p8u1\n",
      "afp.20000915.0017:p3u1\n",
      "assabah.20040928.0023:p74u1\n",
      "assabah.20041003.0016:p3u1\n",
      "assabah.20041004.0011:p13u1\n",
      "assabah.20041005.0021:p9u1\n",
      "assabah.20041007.0003:p8u1\n",
      "assabah.20041007.0003:p8u1\n",
      "assabah.20041013.0021:p16u1\n",
      "assabah.20041018.0001:p8u1\n",
      "assabah.20041018.0001:p8u1\n",
      "assabah.20041018.0015:p21u1\n",
      "assabah.20041018.0016:p2u1\n",
      "assabah.20041018.0032:p33u1\n",
      "assabah.20041018.0032:p41u1\n",
      "assabah.20041020.0007:p4u1\n",
      "assabah.20041022.0003:p8u1\n",
      "assabah.20041022.0003:p10u1\n",
      "alhayat.20010617.0114:p5u1\n",
      "alhayat.20010617.0114:p10u1\n",
      "alhayat.20010725.0041:p5u1\n",
      "alhayat.20010725.0041:p5u1\n",
      "alhayat.20010725.0041:p5u1\n",
      "alhayat.20010818.0106:p13u1\n",
      "alhayat.20010911.0001:p4u1\n",
      "alhayat.20010911.0033:p12u1\n",
      "alhayat.20010911.0036:p4u1\n",
      "alhayat.20010912.0055:p3u1\n",
      "alhayat.20010912.0055:p3u1\n",
      "alhayat.20010912.0055:p3u1\n",
      "alhayat.20011106.0051:p12u1\n",
      "alhayat.20011121.0093:p2u1\n",
      "alhayat.20011129.0051:p7u1\n",
      "alhayat.20011129.0051:p7u1\n",
      "alhayat.20011203.0121:p5u1\n",
      "alhayat.20011204.0113:p6u1\n",
      "alhayat.20011211.0123:p6u1\n",
      "alhayat.20011211.0123:p6u1\n",
      "alhayat.20020103.0035:p5u1\n",
      "alhayat.20020103.0035:p8u1\n",
      "alhayat.20020103.0035:p8u1\n",
      "alhayat.20020116.0123:p3u1\n",
      "alhayat.20020116.0123:p8u1\n",
      "alhayat.20020116.0123:p8u1\n",
      "alhayat.20020116.0123:p9u1\n",
      "annahar.20021101.0003:p12u1\n",
      "annahar.20021101.0004:p11u1\n",
      "annahar.20021101.0004:p13u1\n",
      "annahar.20021101.0004:p16u1\n",
      "annahar.20021101.0006:p3u1\n",
      "annahar.20021101.0006:p9u1\n",
      "annahar.20021101.0006:p14u1\n",
      "annahar.20021101.0006:p14u1\n",
      "annahar.20021101.0009:p3u1\n",
      "annahar.20021101.0009:p3u1\n",
      "annahar.20021101.0009:p6u1\n",
      "annahar.20021101.0009:p6u1\n",
      "annahar.20021130.0085:p17u1\n",
      "annahar.20021130.0085:p17u1\n",
      "annahar.20021130.0085:p18u1\n",
      "annahar.20021130.0085:p18u1\n",
      "annahar.20021130.0085:p18u1\n",
      "annahar.20021130.0095:p12u1\n",
      "annahar.20021130.0095:p19u1\n",
      "annahar.20021130.0099:p7u1\n",
      "ummah.20040407.0011:p6u1\n",
      "ummah.20040407.0012:p5u1\n",
      "ummah.20040407.0012:p7u1\n",
      "ummah.20040531.0004:p4u1\n",
      "ummah.20040531.0004:p4u1\n",
      "ummah.20040531.0008:p5u1\n",
      "ummah.20040531.0008:p5u1\n",
      "ummah.20040531.0008:p5u2\n",
      "ummah.20040531.0008:p5u2\n",
      "ummah.20040531.0012:p12u1\n",
      "ummah.20040531.0021:p3u1\n",
      "ummah.20040531.0022:p5u1\n",
      "ummah.20040531.0032:p2u2\n",
      "ummah.20040628.0049:p3u1\n",
      "ummah.20040628.0058:p4u2\n",
      "ummah.20040628.0061:p6u1\n",
      "ummah.20040628.0061:p7u1\n",
      "ummah.20040628.0064:p6u1\n",
      "ummah.20040628.0064:p10u1\n",
      "ummah.20040705.0006:p9u1\n",
      "ummah.20040705.0007:p7u1\n",
      "ummah.20040705.0009:p5u1\n",
      "ummah.20040705.0012:p2u3\n",
      "ummah.20040705.0015:p3u1\n",
      "ummah.20040705.0015:p10u1\n",
      "ummah.20040705.0015:p14u2\n",
      "ummah.20040705.0015:p19u1\n",
      "ummah.20040705.0015:p22u1\n",
      "ummah.20040705.0018:p2u1\n",
      "ummah.20040705.0028:p4u1\n",
      "ummah.20040705.0028:p4u1\n",
      "ummah.20040705.0028:p5u1\n",
      "ummah.20040705.0029:p4u1\n",
      "ummah.20040705.0029:p5u1\n",
      "ummah.20040705.0029:p6u2\n",
      "ummah.20040705.0029:p6u2\n",
      "ummah.20040705.0031:p3u1\n",
      "ummah.20040705.0031:p7u1\n",
      "ummah.20040715.0003:p10u1\n",
      "ummah.20040715.0003:p12u1\n",
      "ummah.20040715.0009:p3u2\n",
      "ummah.20040715.0014:p5u1\n",
      "ummah.20040726.0003:p3u1\n",
      "ummah.20040726.0018:p9u1\n",
      "ummah.20040726.0022:p2u2\n",
      "ummah.20040726.0049:p4u1\n",
      "ummah.20040726.0051:p3u1\n",
      "ummah.20040726.0056:p7u1\n",
      "ummah.20040726.0073:p6u2\n",
      "ummah.20040726.0073:p8u1\n",
      "ummah.20040809.0069:p10u2\n",
      "ummah.20040809.0084:p7u3\n",
      "ummah.20040809.0084:p8u1\n",
      "ummah.20040809.0085:p6u2\n",
      "ummah.20040809.0086:p6u1\n",
      "ummah.20040809.0086:p7u1\n",
      "ummah.20040809.0086:p7u1\n",
      "ummah.20040819.0004:p11u2\n",
      "ummah.20040819.0005:p7u2\n",
      "ummah.20040819.0005:p7u2\n",
      "ummah.20040819.0005:p7u2\n",
      "ummah.20040819.0005:p8u1\n",
      "ummah.20040819.0005:p9u1\n",
      "ummah.20040819.0007:p4u1\n",
      "ummah.20040819.0007:p4u2\n",
      "ummah.20040819.0007:p5u2\n",
      "ummah.20040819.0009:p3u2\n",
      "ummah.20040819.0010:p1u1\n",
      "ummah.20040819.0011:p2u1\n",
      "ummah.20040819.0012:p2u1\n",
      "ummah.20040819.0013:p4u3\n",
      "ummah.20040819.0021:p6u1\n",
      "ummah.20040819.0021:p6u1\n",
      "ummah.20040819.0027:p3u1\n",
      "ummah.20040819.0033:p11u1\n",
      "ummah.20040819.0038:p2u1\n",
      "ummah.20040819.0039:p8u1\n",
      "ummah.20040906.0008:p3u2\n",
      "ummah.20040906.0012:p7u1\n",
      "ummah.20040906.0012:p7u1\n",
      "ummah.20040906.0024:p5u1\n",
      "ummah.20040906.0026:p11u1\n",
      "ummah.20040906.0026:p11u1\n",
      "ummah.20040906.0030:p2u1\n",
      "ummah.20040906.0030:p4u1\n",
      "ummah.20040906.0031:p10u1\n",
      "xinhua.20030501.0001:p5u1\n",
      "xinhua.20030503.0003:p6u1\n",
      "xinhua.20030503.0048:p8u1\n",
      "xinhua.20030503.0128:p7u1\n",
      "xinhua.20030503.0129:p5u1\n",
      "xinhua.20030503.0129:p5u1\n",
      "xinhua.20030503.0133:p4u1\n",
      "xinhua.20030503.0136:p8u1\n",
      "xinhua.20030503.0157:p5u1\n",
      "xinhua.20030503.0163:p6u1\n",
      "xinhua.20030503.0170:p5u1\n",
      "xinhua.20030503.0170:p5u1\n",
      "xinhua.20030503.0175:p4u1\n",
      "xinhua.20030503.0178:p8u1\n",
      "xinhua.20030503.0194:p9u1\n",
      "xinhua.20030509.0144:p8u1\n",
      "xinhua.20030509.0144:p9u1\n",
      "xinhua.20030509.0148:p7u1\n",
      "xinhua.20030509.0156:p4u1\n",
      "xinhua.20030509.0161:p6u1\n",
      "xinhua.20030509.0161:p6u1\n",
      "xinhua.20030509.0162:p3u1\n",
      "xinhua.20030509.0164:p4u1\n",
      "xinhua.20030511.0191:p7u1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train = pyconll.load_from_file('Arabic_POS.conllu')\n",
    "\n",
    "review_sentences = []\n",
    "\n",
    "# Conll objects are iterable over their sentences, and sentences are iterable\n",
    "# over their tokens. Sentences also de/serialize comment information.\n",
    "for sentence in train:\n",
    "   for token in sentence:\n",
    "\n",
    "   # Tokens have attributes such as upos, head, id, deprel, etc, and sentences\n",
    "   # can be indexed by a token's id. We must check that the token is not the\n",
    "   # root token, whose id, '0', cannot be looked up.\n",
    "    if token.upos == 'AUX' and (token.head != '0' and sentence[token.head].upos == 'NOUN'):\n",
    "      review_sentences.append(sentence)\n",
    "\n",
    "print('Review the following sentences:')\n",
    "for sent in review_sentences:\n",
    "   print(sent.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "705f4c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyconll.unit.conll.Conll at 0x1a43cdbdad0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pyconll.load_from_file('Arabic_POS.conllu')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e8d43f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyconll.unit.conll.Conll object at 0x000001A43CDBDAD0>\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c718339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6075\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "327969ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "برلين\tX\n",
      "ترفض\tVERB\n",
      "حصول\tNOUN\n",
      "شركة\tNOUN\n",
      "اميركية\tADJ\n",
      "على\tADP\n",
      "رخصة\tNOUN\n",
      "تصنيع\tNOUN\n",
      "دبابة\tNOUN\n",
      "\"\tPUNCT\n",
      "ليوبارد\tX\n",
      "\"\tPUNCT\n",
      "الالمانية\tADJ\n"
     ]
    }
   ],
   "source": [
    "for token in data[0]:\n",
    "    print(f\"{token.form}\\t{token.upos}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260f2634",
   "metadata": {},
   "source": [
    "| Tag   | Meaning            |\n",
    "| ----- | ------------------ |\n",
    "| NOUN  | Noun               |\n",
    "| VERB  | Verb               |\n",
    "| ADJ   | Adjective          |\n",
    "| ADV   | Adverb             |\n",
    "| PRON  | Pronoun            |\n",
    "| DET   | Determiner         |\n",
    "| ADP   | Adposition (prep)  |\n",
    "| AUX   | Auxiliary verb     |\n",
    "| CCONJ | Coordinating conj  |\n",
    "| SCONJ | Subordinating conj |\n",
    "| PUNCT | Punctuation        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d56ee454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prepare the CSV file\n",
    "# with open('arabic_pos_tags.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "#     writer = csv.writer(file)\n",
    "    \n",
    "#     # Write header row\n",
    "#     writer.writerow(['Sentence_ID', 'Word', 'POS_Tag'])\n",
    "    \n",
    "#     # Iterate through the sentences and tokens\n",
    "#     for sentence in data:\n",
    "#         for token in sentence:\n",
    "#             # Write the sentence ID, word (form), and POS tag (upos) to the CSV file\n",
    "#             writer.writerow([sentence.id, token.form, token.upos])\n",
    "\n",
    "# print(\"POS tagging results saved to arabic_pos_tags.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95f4d071",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence_ID</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS_Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afp.20000715.0075:p1u1</td>\n",
       "      <td>برلين</td>\n",
       "      <td>X</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afp.20000715.0075:p1u1</td>\n",
       "      <td>ترفض</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afp.20000715.0075:p1u1</td>\n",
       "      <td>حصول</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>afp.20000715.0075:p1u1</td>\n",
       "      <td>شركة</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afp.20000715.0075:p1u1</td>\n",
       "      <td>اميركية</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254395</th>\n",
       "      <td>xinhua.20030511.0192:p7u1</td>\n",
       "      <td>بنية</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254396</th>\n",
       "      <td>xinhua.20030511.0192:p7u1</td>\n",
       "      <td>ه</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254397</th>\n",
       "      <td>xinhua.20030511.0192:p7u1</td>\n",
       "      <td>التحتية</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254398</th>\n",
       "      <td>xinhua.20030511.0192:p7u1</td>\n",
       "      <td>\"</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254399</th>\n",
       "      <td>xinhua.20030511.0192:p7u1</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>254400 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Sentence_ID     Word POS_Tag\n",
       "0          afp.20000715.0075:p1u1    برلين       X\n",
       "1          afp.20000715.0075:p1u1     ترفض    VERB\n",
       "2          afp.20000715.0075:p1u1     حصول    NOUN\n",
       "3          afp.20000715.0075:p1u1     شركة    NOUN\n",
       "4          afp.20000715.0075:p1u1  اميركية     ADJ\n",
       "...                           ...      ...     ...\n",
       "254395  xinhua.20030511.0192:p7u1     بنية    NOUN\n",
       "254396  xinhua.20030511.0192:p7u1        ه    PRON\n",
       "254397  xinhua.20030511.0192:p7u1  التحتية     ADJ\n",
       "254398  xinhua.20030511.0192:p7u1        \"   PUNCT\n",
       "254399  xinhua.20030511.0192:p7u1        .   PUNCT\n",
       "\n",
       "[254400 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_csv=pd.read_csv(\"arabic_pos_tags.csv\")\n",
    "data_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60796b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "sentences = []\n",
    "labels = []\n",
    "\n",
    "for sentence in data:\n",
    "    words = []\n",
    "    tags = []\n",
    "    for token in sentence:\n",
    "        if token.upos:\n",
    "            words.append(token.form)\n",
    "            tags.append(token.upos)\n",
    "    if words:\n",
    "        sentences.append(words)\n",
    "        labels.append(tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c53074f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['برلين', 'ترفض', 'حصول', 'شركة', 'اميركية', 'على', 'رخصة', 'تصنيع', 'دبابة', '\"', 'ليوبارد', '\"', 'الالمانية']\n",
      "['X', 'VERB', 'NOUN', 'NOUN', 'ADJ', 'ADP', 'NOUN', 'NOUN', 'NOUN', 'PUNCT', 'X', 'PUNCT', 'ADJ']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa08c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(sentence, index):\n",
    "    word = sentence[index]\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'prefix': word[:2],\n",
    "        'suffix': word[-2:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# Transform data into features and labels\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for sentence, tag_seq in zip(sentences, labels):\n",
    "    for i in range(len(sentence)):\n",
    "        X.append(extract_features(sentence, i))\n",
    "        y.append(tag_seq[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4194a786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'ترفض', 'prefix': 'تر', 'suffix': 'فض', 'prev_word': 'برلين', 'next_word': 'حصول'}\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "print(X[1])\n",
    "print(y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1a3ce87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained GloVe embeddings\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "def extract_features_with_embeddings(sentence, index):\n",
    "    word = sentence[index]\n",
    "    \n",
    "    # Word features (previous features)\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'prefix': word[:2],\n",
    "        'suffix': word[-2:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "    }\n",
    "    \n",
    "    # Add word embedding as feature (return a zero vector if not found)\n",
    "    word_embedding = word_vectors[word] if word in word_vectors else np.zeros(100)\n",
    "    features['word_embedding'] = word_embedding\n",
    "    \n",
    "    return features\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for sentence, tag_seq in zip(sentences, labels):\n",
    "    for i in range(len(sentence)):\n",
    "        X.append(extract_features_with_embeddings(sentence, i))\n",
    "        y.append(tag_seq[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3bca872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'ترفض', 'prefix': 'تر', 'suffix': 'فض', 'prev_word': 'برلين', 'next_word': 'حصول', 'word_embedding': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])}\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "print(X[1])\n",
    "print(y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0eb3983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def extract_char_ngrams(sentence, index, n=3):\n",
    "    word = sentence[index]\n",
    "    \n",
    "    # Generate character n-grams\n",
    "    char_ngrams = [word[i:i+n] for i in range(len(word)-n+1)]\n",
    "    return char_ngrams\n",
    "\n",
    "def extract_features_with_char_ngrams(sentence, index):\n",
    "    word = sentence[index]\n",
    "    \n",
    "    # Extract features (previous features)\n",
    "    features = {\n",
    "        'word': word,\n",
    "        'prefix': word[:1],\n",
    "        'suffix': word[-2:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "    }\n",
    "    \n",
    "    # Extract character-level n-grams\n",
    "    char_ngrams = extract_char_ngrams(sentence, index, n=3)  # trigram\n",
    "    features['char_ngrams'] = char_ngrams\n",
    "    \n",
    "    return features\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for sentence, tag_seq in zip(sentences, labels):\n",
    "    for i in range(len(sentence)):\n",
    "        X.append(extract_features_with_char_ngrams(sentence, i))\n",
    "        y.append(tag_seq[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bd4d8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'ترفض', 'prefix': 'ت', 'suffix': 'فض', 'prev_word': 'برلين', 'next_word': 'حصول', 'char_ngrams': ['ترف', 'رفض']}\n",
      "VERB\n"
     ]
    }
   ],
   "source": [
    "print(X[1])\n",
    "print(y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3a515d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.15,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57e24180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65d2c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = DictVectorizer(sparse=True)\n",
    "X_train_vectorized = vectorizer.fit_transform(x_train)\n",
    "X_test_vectorized = vectorizer.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "007fa1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=200)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=200)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=200)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression(max_iter=200)\n",
    "classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36496a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.980047084047126"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(X_train_vectorized,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d060ecb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9539052496798975"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(X_test_vectorized,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a6df3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c520513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.92      0.92      0.92      3509\n",
      "         ADP       0.99      1.00      0.99      4991\n",
      "         ADV       0.97      0.83      0.90       114\n",
      "         AUX       0.90      0.87      0.89       245\n",
      "       CCONJ       0.99      0.99      0.99      2384\n",
      "         DET       0.95      0.98      0.96       698\n",
      "        INTJ       0.00      0.00      0.00         3\n",
      "        NOUN       0.94      0.97      0.95     11346\n",
      "         NUM       0.99      0.97      0.98       884\n",
      "        PART       0.94      0.89      0.91       254\n",
      "        PRON       0.99      0.99      0.99      1286\n",
      "       PROPN       1.00      0.69      0.81        32\n",
      "       PUNCT       1.00      1.00      1.00      2646\n",
      "       SCONJ       0.97      0.99      0.98       647\n",
      "         SYM       1.00      0.96      0.98        54\n",
      "        VERB       0.94      0.95      0.95      2457\n",
      "           X       0.88      0.69      0.77      2033\n",
      "\n",
      "    accuracy                           0.95     33583\n",
      "   macro avg       0.90      0.86      0.88     33583\n",
      "weighted avg       0.95      0.95      0.95     33583\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\dell\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "y_pred = classifier.predict(X_test_vectorized)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc51852c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOUN']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Example word\n",
    "word = \"فوق\"\n",
    "sentence = [word]  # Single-word sentence\n",
    "index = 0\n",
    "\n",
    "# Step 1: Extract features\n",
    "features = extract_features(sentence, index)\n",
    "\n",
    "# Step 2: Vectorize features\n",
    "X_test = vectorizer.transform([features])  # Note: list of 1 dict\n",
    "\n",
    "# Step 3: Predict\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c79123d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOUN' 'ADP' 'VERB' 'NOUN' 'DET']\n"
     ]
    }
   ],
   "source": [
    "# List of Arabic words\n",
    "words = [\"فوق\", \"تحت\", \"يكتب\", \"اللبن\", \"التي\"]\n",
    "\n",
    "# Prepare features for each word\n",
    "test_features = [extract_features([word], 0) for word in words]\n",
    "\n",
    "# Vectorize the features\n",
    "X_test = vectorizer.transform(test_features)\n",
    "\n",
    "# Predict POS tags\n",
    "y_preds = classifier.predict(X_test)\n",
    "\n",
    "print(y_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cfb4fc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\python311\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\python311\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\python311\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\python311\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\python311\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2023.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python311\\lib\\site-packages (from requests->transformers) (2023.5.7)\n",
      "Using cached transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "Using cached huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.26.0\n",
      "    Uninstalling huggingface-hub-0.26.0:\n",
      "      Successfully uninstalled huggingface-hub-0.26.0\n",
      "  Rolling back uninstall of huggingface-hub\n",
      "  Moving to c:\\users\\dell\\appdata\\roaming\\python\\python311\\scripts\\huggingface-cli.exe\n",
      "   from C:\\Users\\dell\\AppData\\Local\\Temp\\pip-uninstall-341vyock\\huggingface-cli.exe\n",
      "  Moving to c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages\\huggingface_hub-0.26.0.dist-info\\\n",
      "   from C:\\Users\\dell\\AppData\\Roaming\\Python\\Python311\\site-packages\\~uggingface_hub-0.26.0.dist-info\n",
      "  Moving to c:\\users\\dell\\appdata\\roaming\\python\\python311\\site-packages\\huggingface_hub\\\n",
      "   from C:\\Users\\dell\\AppData\\Roaming\\Python\\Python311\\site-packages\\~uggingface_hub\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~%p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~-p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~0p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~1p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~2p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~3p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~4p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~5p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~6p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~7p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~8p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~=p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~~p (c:\\Python311\\Lib\\site-packages)\n",
      "DEPRECATION: Loading egg at c:\\python311\\lib\\site-packages\\vboxapi-1.0-py3.11.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "WARNING: Ignoring invalid distribution ~%p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~-p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~0p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~1p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~2p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~3p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~4p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~5p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~6p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~7p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~8p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~=p (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~atplotlib (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (c:\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~~p (c:\\Python311\\Lib\\site-packages)\n",
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] The system cannot find the file specified: 'c:\\\\Python311\\\\Scripts\\\\huggingface-cli.exe' -> 'c:\\\\Python311\\\\Scripts\\\\huggingface-cli.exe.deleteme'\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7eb91b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b60ed28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d77d0f10adc41f6bcdecbd2f5eb6f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at asafaya/bert-base-arabic were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"asafaya/bert-base-arabic\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"asafaya/bert-base-arabic\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd7de186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "df423733",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ذهبت → LABEL_1\n",
      "الى → LABEL_0\n",
      "المدرسة → LABEL_0\n",
      "في → LABEL_1\n",
      "الصباح → LABEL_1\n",
      ". → LABEL_0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# Use a public model for token classification\n",
    "model_name = \"asafaya/bert-base-arabic\"  # Public model\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)  # This may not be trained for POS\n",
    "\n",
    "# Build a dummy pipeline (useful only for testing)\n",
    "pos_tagger = pipeline(\"token-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "sentence = \"ذهبت إلى المدرسة في الصباح.\"\n",
    "results = pos_tagger(sentence)\n",
    "\n",
    "# Display token and its predicted tag\n",
    "for res in results:\n",
    "    print(f\"{res['word']} → {res['entity']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d98825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb61488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3569843d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfea128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d0bec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410224c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac4a674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d237bc5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574521ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06052ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedfd8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee19a098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# import numpy as np\n",
    "# from collections import defaultdict\n",
    "# from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "# from camel_tools.morphology.database import MorphologyDB\n",
    "# from camel_tools.morphology.analyzer import Analyzer\n",
    "# from gensim.models import KeyedVectors\n",
    "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# # Load Arabic morphological analyzer\n",
    "# morph_db = MorphologyDB.builtin_db()\n",
    "# analyzer = Analyzer(morph_db)\n",
    "\n",
    "# # Function to normalize Arabic text\n",
    "# def normalize_arabic(text):\n",
    "#     \"\"\"Apply basic Arabic text normalization\"\"\"\n",
    "#     text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "#     text = re.sub(\"ى\", \"ي\", text)\n",
    "#     text = re.sub(\"ة\", \"ه\", text)\n",
    "#     text = re.sub(\"گ\", \"ك\", text)\n",
    "#     text = re.sub(\"ؤ\", \"ء\", text)\n",
    "#     text = re.sub(\"ئ\", \"ء\", text)\n",
    "#     return text\n",
    "\n",
    "# # Enhanced feature extraction for Arabic words\n",
    "# def extract_rich_features(sentence, index, context_size=2):\n",
    "#     \"\"\"\n",
    "#     Extract rich linguistic features for Arabic POS tagging.\n",
    "    \n",
    "#     Args:\n",
    "#         sentence: List of tokenized Arabic words\n",
    "#         index: Current word position\n",
    "#         context_size: Window size for surrounding words\n",
    "    \n",
    "#     Returns:\n",
    "#         Dictionary of features\n",
    "#     \"\"\"\n",
    "#     word = sentence[index]\n",
    "#     normalized_word = normalize_arabic(word)\n",
    "    \n",
    "#     # Get morphological analysis\n",
    "#     analyses = analyzer.analyze(word)\n",
    "    \n",
    "#     # Get context words with boundary handling\n",
    "#     context = []\n",
    "#     for i in range(index - context_size, index + context_size + 1):\n",
    "#         if i == index:  # Skip the current word\n",
    "#             continue\n",
    "#         if 0 <= i < len(sentence):\n",
    "#             context.append(normalize_arabic(sentence[i]))\n",
    "#         else:\n",
    "#             context.append(\"<PAD>\")\n",
    "    \n",
    "#     # Extract features\n",
    "#     features = {\n",
    "#         # Word-level features\n",
    "#         'word': normalized_word,\n",
    "#         'word_length': len(word),\n",
    "#         'has_digit': bool(re.search(r'\\d', word)),\n",
    "#         'has_arabic_digit': bool(re.search(r'[٠-٩]', word)),\n",
    "#         'is_digit': word.isdigit() or all(c in '٠١٢٣٤٥٦٧٨٩' for c in word),\n",
    "        \n",
    "#         # Morphological features\n",
    "#         'prefixes': normalized_word[:3],\n",
    "#         'suffixes': normalized_word[-3:],\n",
    "#         'stem': analyses[0]['stem'] if analyses else word,  # Most likely stem\n",
    "#         'pos_options': [a.get('pos', '') for a in analyses],\n",
    "#         'gender': [a.get('gen', '') for a in analyses if 'gen' in a],\n",
    "#         'number': [a.get('num', '') for a in analyses if 'num' in a],\n",
    "#         'has_definite_article': any('Al+' in a.get('bw', '') for a in analyses),\n",
    "#         'has_attached_pronoun': any('PRON' in a.get('bw', '') for a in analyses),\n",
    "        \n",
    "#         # Context features\n",
    "#         'prev_words': context[:context_size],\n",
    "#         'next_words': context[context_size:],\n",
    "#         'position_in_sentence': index / len(sentence)  # Normalized position\n",
    "#     }\n",
    "    \n",
    "#     return features\n",
    "\n",
    "# # Process dataset with rich features\n",
    "# def prepare_dataset_with_rich_features(text_data, labels, max_seq_length=100):\n",
    "#     \"\"\"\n",
    "#     Convert text data and labels into feature matrices suitable for model training\n",
    "    \n",
    "#     Args:\n",
    "#         text_data: List of tokenized sentences\n",
    "#         labels: List of POS tag sequences\n",
    "#         max_seq_length: Maximum sequence length for padding\n",
    "    \n",
    "#     Returns:\n",
    "#         X_features: Feature matrices\n",
    "#         y_encoded: Encoded labels\n",
    "#     \"\"\"\n",
    "#     # Create tag to index mapping\n",
    "#     tag2idx = {tag: idx for idx, tag in enumerate(sorted(set([t for seq in labels for t in seq])))}\n",
    "#     idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
    "    \n",
    "#     # Feature extraction for each sentence\n",
    "#     all_features = []\n",
    "#     all_tags = []\n",
    "    \n",
    "#     for sentence, tag_seq in zip(text_data, labels):\n",
    "#         sent_features = []\n",
    "#         for i in range(len(sentence)):\n",
    "#             features = extract_rich_features(sentence, i)\n",
    "#             sent_features.append(features)\n",
    "        \n",
    "#         # Only keep sequences that fit within max_seq_length\n",
    "#         if len(sent_features) <= max_seq_length:\n",
    "#             all_features.append(sent_features)\n",
    "#             # Convert tags to indices\n",
    "#             sent_tags = [tag2idx[tag] for tag in tag_seq]\n",
    "#             all_tags.append(sent_tags)\n",
    "    \n",
    "#     # Pad sequences\n",
    "#     X_padded = pad_sequences([[1] * len(f) for f in all_features], maxlen=max_seq_length, padding='post')\n",
    "#     y_padded = pad_sequences(all_tags, maxlen=max_seq_length, padding='post', value=tag2idx.get('O', 0))\n",
    "    \n",
    "#     # One-hot encode the labels\n",
    "#     y_encoded = to_categorical(y_padded, num_classes=len(tag2idx))\n",
    "    \n",
    "#     return X_padded, y_encoded, tag2idx, idx2tag\n",
    "\n",
    "# # Example of embedding-based features (if using word embeddings)\n",
    "# def integrate_word_embeddings(sentences, word_embeddings_path, embedding_dim=300):\n",
    "#     \"\"\"\n",
    "#     Integrate pre-trained Arabic word embeddings\n",
    "    \n",
    "#     Args:\n",
    "#         sentences: List of tokenized sentences\n",
    "#         word_embeddings_path: Path to pre-trained embeddings\n",
    "#         embedding_dim: Dimensionality of embeddings\n",
    "    \n",
    "#     Returns:\n",
    "#         Embedding matrix for vocabulary\n",
    "#     \"\"\"\n",
    "#     # Load pre-trained word vectors\n",
    "#     word_vectors = KeyedVectors.load_word2vec_format(word_embeddings_path, binary=False)\n",
    "    \n",
    "#     # Build vocabulary from data\n",
    "#     word_set = set()\n",
    "#     for sentence in sentences:\n",
    "#         for word in sentence:\n",
    "#             word_set.add(normalize_arabic(word))\n",
    "    \n",
    "#     # Create word to index mapping\n",
    "#     word2idx = {word: idx + 2 for idx, word in enumerate(sorted(word_set))}\n",
    "#     word2idx['<PAD>'] = 0\n",
    "#     word2idx['<UNK>'] = 1\n",
    "    \n",
    "#     # Initialize embedding matrix\n",
    "#     embedding_matrix = np.zeros((len(word2idx), embedding_dim))\n",
    "    \n",
    "#     # Fill embedding matrix\n",
    "#     for word, idx in word2idx.items():\n",
    "#         if word in word_vectors:\n",
    "#             embedding_matrix[idx] = word_vectors[word]\n",
    "#         elif word != '<PAD>':\n",
    "#             # Random initialization for unknown words\n",
    "#             embedding_matrix[idx] = np.random.uniform(-0.25, 0.25, embedding_dim)\n",
    "    \n",
    "#     return embedding_matrix, word2idx\n",
    "\n",
    "# # Example usage:\n",
    "# \"\"\"\n",
    "# # Create feature arrays for BiLSTM-CRF model\n",
    "# X_train, y_train, tag2idx, idx2tag = prepare_dataset_with_rich_features(train_sentences, train_tags)\n",
    "# X_valid, y_valid, _, _ = prepare_dataset_with_rich_features(valid_sentences, valid_tags, tag2idx)\n",
    "\n",
    "# # If using word embeddings:\n",
    "# embedding_matrix, word2idx = integrate_word_embeddings(\n",
    "#     train_sentences + valid_sentences,\n",
    "#     'path/to/arabic/word2vec.txt'\n",
    "# )\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ed458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Arabic text normalization function\n",
    "# def normalize_arabic(text):\n",
    "#     \"\"\"Apply basic Arabic text normalization\"\"\"\n",
    "#     text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "#     text = re.sub(\"ى\", \"ي\", text)\n",
    "#     text = re.sub(\"ة\", \"ه\", text)\n",
    "#     text = re.sub(\"گ\", \"ك\", text)\n",
    "#     text = re.sub(\"ؤ\", \"ء\", text)\n",
    "#     text = re.sub(\"ئ\", \"ء\", text)\n",
    "#     return text\n",
    "\n",
    "# def extract_stem(word):\n",
    "#     \"\"\"Basic rule-based stemming for Arabic\"\"\"\n",
    "#     # Common prefixes to remove (from longest to shortest)\n",
    "#     prefixes = ['است', 'الم', 'وال', 'بال', 'كال', 'فال', 'ال', 'لل', 'و', 'ف', 'ب', 'ك', 'ل', 'س']\n",
    "    \n",
    "#     # Common suffixes to remove (from longest to shortest)\n",
    "#     suffixes = ['تمونا', 'تموها', 'تموه', 'ناها', 'ناه', 'تما', 'تم', 'تن', 'نا', 'ون', 'ات', 'ين', 'تان', 'ان', 'ها', 'هم', 'هن', 'كما', 'كم', 'كن', 'ه', 'ي', 'ك', 'ة', 'ت']\n",
    "    \n",
    "#     stem = word\n",
    "    \n",
    "#     # Try to find the longest matching prefix\n",
    "#     for prefix in prefixes:\n",
    "#         if stem.startswith(prefix) and len(stem) - len(prefix) >= 2:\n",
    "#             stem = stem[len(prefix):]\n",
    "#             break\n",
    "    \n",
    "#     # Try to find the longest matching suffix\n",
    "#     for suffix in suffixes:\n",
    "#         if stem.endswith(suffix) and len(stem) - len(suffix) >= 2:\n",
    "#             stem = stem[:-len(suffix)]\n",
    "#             break\n",
    "    \n",
    "#     return stem\n",
    "\n",
    "# # Feature extraction function\n",
    "# def extract_features(sentence, index, context_size=2):\n",
    "#     \"\"\"Extract rich linguistic features for Arabic words\"\"\"\n",
    "#     word = sentence[index]\n",
    "#     normalized_word = normalize_arabic(word)\n",
    "    \n",
    "#     # Get context words with boundary handling\n",
    "#     context = []\n",
    "#     for i in range(index - context_size, index + context_size + 1):\n",
    "#         if i == index:  # Skip the current word\n",
    "#             continue\n",
    "#         if 0 <= i < len(sentence):\n",
    "#             context.append(normalize_arabic(sentence[i]))\n",
    "#         else:\n",
    "#             context.append(\"<PAD>\")\n",
    "    \n",
    "#     # Extract basic stem\n",
    "#     stem = extract_stem(normalized_word)\n",
    "    \n",
    "#     # Morphological patterns detection\n",
    "#     has_definite_article = any(normalized_word.startswith(prefix) for prefix in ['ال', 'وال', 'بال', 'كال', 'فال', 'لل'])\n",
    "#     has_feminine_marker = normalized_word.endswith('ة') or normalized_word.endswith('ات')\n",
    "#     has_plural_marker = normalized_word.endswith('ون') or normalized_word.endswith('ين') or normalized_word.endswith('ات')\n",
    "    \n",
    "#     # Extract features as dictionary\n",
    "#     features = {\n",
    "#         # Word-level features\n",
    "#         'word': normalized_word,\n",
    "#         'word_length': len(word),\n",
    "#         'prefix_1': normalized_word[:1] if len(normalized_word) > 0 else '',\n",
    "#         'prefix_2': normalized_word[:2] if len(normalized_word) > 1 else '',\n",
    "#         'prefix_3': normalized_word[:3] if len(normalized_word) > 2 else '',\n",
    "#         'suffix_1': normalized_word[-1:] if len(normalized_word) > 0 else '',\n",
    "#         'suffix_2': normalized_word[-2:] if len(normalized_word) > 1 else '',\n",
    "#         'suffix_3': normalized_word[-3:] if len(normalized_word) > 2 else '',\n",
    "#         'stem': stem,\n",
    "#         'is_digit': word.isdigit() or all(c in '٠١٢٣٤٥٦٧٨٩' for c in word),\n",
    "        \n",
    "#         # Morphological features\n",
    "#         'has_definite_article': has_definite_article,\n",
    "#         'has_feminine_marker': has_feminine_marker,\n",
    "#         'has_plural_marker': has_plural_marker,\n",
    "        \n",
    "#         # Context features\n",
    "#         'prev_word': context[context_size-1] if context_size > 0 and len(context) >= context_size else '',\n",
    "#         'next_word': context[context_size] if len(context) > context_size else ''\n",
    "#     }\n",
    "    \n",
    "#     return features\n",
    "\n",
    "# # Example data preparation\n",
    "# def prepare_data(sentences, labels):\n",
    "#     \"\"\"\n",
    "#     Prepare data for training - extract features and convert labels\n",
    "    \n",
    "#     Args:\n",
    "#         sentences: List of tokenized sentences\n",
    "#         labels: List of POS tag sequences\n",
    "    \n",
    "#     Returns:\n",
    "#         X_features: Feature matrices\n",
    "#         y_labels: Encoded labels\n",
    "#         vectorizer: DictVectorizer for feature transformation\n",
    "#         tag2idx: Dictionary mapping tags to indices\n",
    "#     \"\"\"\n",
    "#     # Create tag mapping\n",
    "#     all_tags = set([tag for sent_tags in labels for tag in sent_tags])\n",
    "#     tag2idx = {tag: idx for idx, tag in enumerate(sorted(all_tags))}\n",
    "#     idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
    "    \n",
    "#     # Extract features\n",
    "#     X_dict = []\n",
    "#     y = []\n",
    "    \n",
    "#     for sentence, sent_tags in zip(sentences, labels):\n",
    "#         for i in range(len(sentence)):\n",
    "#             X_dict.append(extract_features(sentence, i))\n",
    "#             y.append(tag2idx[sent_tags[i]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82b776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# from collections import defaultdict\n",
    "\n",
    "# # Arabic text normalization function\n",
    "# def normalize_arabic(text):\n",
    "#     \"\"\"Apply basic Arabic text normalization\"\"\"\n",
    "#     text = re.sub(\"[إأآا]\", \"ا\", text)  # Normalize different forms of 'a'\n",
    "#     text = re.sub(\"ى\", \"ي\", text)      # Replace 'ى' with 'ي'\n",
    "#     text = re.sub(\"ة\", \"ه\", text)      # Replace 'ة' with 'ه'\n",
    "#     text = re.sub(\"گ\", \"ك\", text)      # Replace 'گ' with 'ك'\n",
    "#     text = re.sub(\"ؤ\", \"ء\", text)      # Replace 'ؤ' with 'ء'\n",
    "#     text = re.sub(\"ئ\", \"ء\", text)      # Replace 'ئ' with 'ء'\n",
    "#     return text\n",
    "\n",
    "# # Improved stemming function with more customization\n",
    "# def extract_stem(word, advanced_stemmer=None):\n",
    "#     \"\"\"Apply advanced stemming or basic rule-based stemming for Arabic\"\"\"\n",
    "#     prefixes = ['است', 'الم', 'وال', 'بال', 'كال', 'فال', 'ال', 'لل', 'و', 'ف', 'ب', 'ك', 'ل', 'س']\n",
    "#     suffixes = ['تمونا', 'تموها', 'تموه', 'ناها', 'ناه', 'تما', 'تم', 'تن', 'نا', 'ون', 'ات', 'ين', 'تان', 'ان', 'ها', 'هم', 'هن', 'كما', 'كم', 'كن', 'ه', 'ي', 'ك', 'ة', 'ت']\n",
    "    \n",
    "#     # Use advanced stemmer if provided (e.g., from camel_tools or farasa)\n",
    "#     if advanced_stemmer:\n",
    "#         return advanced_stemmer.stem(word)\n",
    "    \n",
    "#     # Otherwise apply the basic rule-based stemming\n",
    "#     stem = word\n",
    "#     for prefix in prefixes:\n",
    "#         if stem.startswith(prefix) and len(stem) - len(prefix) >= 2:\n",
    "#             stem = stem[len(prefix):]\n",
    "#             break\n",
    "    \n",
    "#     for suffix in suffixes:\n",
    "#         if stem.endswith(suffix) and len(stem) - len(suffix) >= 2:\n",
    "#             stem = stem[:-len(suffix)]\n",
    "#             break\n",
    "    \n",
    "#     return stem\n",
    "\n",
    "# # Feature extraction function with more customizable options\n",
    "# def extract_features(sentence, index, context_size=2, advanced_stemmer=None):\n",
    "#     \"\"\"Extract rich linguistic features for Arabic words\"\"\"\n",
    "#     word = sentence[index]\n",
    "#     normalized_word = normalize_arabic(word)\n",
    "    \n",
    "#     # Get context words with boundary handling\n",
    "#     context = []\n",
    "#     for i in range(index - context_size, index + context_size + 1):\n",
    "#         if i == index:  # Skip the current word\n",
    "#             continue\n",
    "#         if 0 <= i < len(sentence):\n",
    "#             context.append(normalize_arabic(sentence[i]))\n",
    "#         else:\n",
    "#             context.append(\"<PAD>\")\n",
    "    \n",
    "#     # Extract basic stem (can integrate advanced stemming here)\n",
    "#     stem = extract_stem(normalized_word, advanced_stemmer)\n",
    "    \n",
    "#     # Morphological patterns detection\n",
    "#     has_definite_article = any(normalized_word.startswith(prefix) for prefix in ['ال', 'وال', 'بال', 'كال', 'فال', 'لل'])\n",
    "#     has_feminine_marker = normalized_word.endswith('ة') or normalized_word.endswith('ات')\n",
    "#     has_plural_marker = normalized_word.endswith('ون') or normalized_word.endswith('ين') or normalized_word.endswith('ات')\n",
    "    \n",
    "#     # Extract features as dictionary\n",
    "#     features = {\n",
    "#         'word': normalized_word,\n",
    "#         'word_length': len(word),\n",
    "#         'prefix_1': normalized_word[:1] if len(normalized_word) > 0 else '',\n",
    "#         'prefix_2': normalized_word[:2] if len(normalized_word) > 1 else '',\n",
    "#         'prefix_3': normalized_word[:3] if len(normalized_word) > 2 else '',\n",
    "#         'suffix_1': normalized_word[-1:] if len(normalized_word) > 0 else '',\n",
    "#         'suffix_2': normalized_word[-2:] if len(normalized_word) > 1 else '',\n",
    "#         'suffix_3': normalized_word[-3:] if len(normalized_word) > 2 else '',\n",
    "#         'stem': stem,\n",
    "#         'is_digit': word.isdigit() or all(c in '٠١٢٣٤٥٦٧٨٩' for c in word),\n",
    "#         'has_definite_article': has_definite_article,\n",
    "#         'has_feminine_marker': has_feminine_marker,\n",
    "#         'has_plural_marker': has_plural_marker,\n",
    "#         'prev_word': context[context_size-1] if context_size > 0 and len(context) >= context_size else '',\n",
    "#         'next_word': context[context_size] if len(context) > context_size else ''\n",
    "#     }\n",
    "    \n",
    "#     return features\n",
    "\n",
    "# # Example of preparing data with custom feature extraction\n",
    "# def prepare_data(sentences, labels, advanced_stemmer=None, context_size=2):\n",
    "#     \"\"\"\n",
    "#     Prepare data for training - extract features and convert labels.\n",
    "    \n",
    "#     Args:\n",
    "#         sentences: List of tokenized sentences.\n",
    "#         labels: List of POS tag sequences.\n",
    "#         advanced_stemmer: Optional stemmer (e.g., from Camel Tools or Farasa).\n",
    "#         context_size: Size of the context window for context features.\n",
    "    \n",
    "#     Returns:\n",
    "#         X_features: Feature matrices.\n",
    "#         y_labels: Encoded labels.\n",
    "#         vectorizer: DictVectorizer for feature transformation.\n",
    "#         tag2idx: Dictionary mapping tags to indices.\n",
    "#     \"\"\"\n",
    "#     # Create tag mapping\n",
    "#     all_tags = set([tag for sent_tags in labels for tag in sent_tags])\n",
    "#     tag2idx = {tag: idx for idx, tag in enumerate(sorted(all_tags))}\n",
    "#     idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
    "    \n",
    "#     # Extract features and prepare the label list\n",
    "#     X_dict = []\n",
    "#     y = []\n",
    "    \n",
    "#     for sentence, sent_tags in zip(sentences, labels):\n",
    "#         for i in range(len(sentence)):\n",
    "#             X_dict.append(extract_features(sentence, i, context_size, advanced_stemmer))\n",
    "#             y.append(tag2idx[sent_tags[i]])\n",
    "    \n",
    "#     return X_dict, y, tag2idx, idx2tag\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd0be7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y, tag2idx, idx2tag = prepare_data(sentences, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707c8978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'word': 'ترفض', 'word_length': 4, 'prefix_1': 'ت', 'prefix_2': 'تر', 'prefix_3': 'ترف', 'suffix_1': 'ض', 'suffix_2': 'فض', 'suffix_3': 'رفض', 'stem': 'ترفض', 'is_digit': False, 'has_definite_article': False, 'has_feminine_marker': False, 'has_plural_marker': False, 'prev_word': 'برلين', 'next_word': 'حصول'}\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "# print(X[1])\n",
    "# print(y[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca61060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4ec9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5899eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dd328d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71e9a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb0bdaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41320912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b21f85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e9632d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6487e73b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc57745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea71564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bc1991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
