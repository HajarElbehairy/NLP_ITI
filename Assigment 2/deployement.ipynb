{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f09b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from collections import Counter\n",
    "import emoji\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e37891b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0</td>\n",
       "      <td>@user i'll always hope that one day i'll get t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "      <td>couple having sex fat naked japanese girls</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0</td>\n",
       "      <td>#hump on that    #hump day #humpersð© @ edwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0</td>\n",
       "      <td>personalised we... gbp 7.99 get here:  #shop #...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                              tweet\n",
       "id                                                           \n",
       "1        0  @user when a father is dysfunctional and is so...\n",
       "2        0  @user @user thanks for #lyft credit i can't us...\n",
       "3        0                                bihday your majesty\n",
       "4        0  #model   i love u take with u all the time in ...\n",
       "5        0             factsguide: society now    #motivation\n",
       "..     ...                                                ...\n",
       "96       0  @user i'll always hope that one day i'll get t...\n",
       "97       0  #model   i love u take with u all the time in ...\n",
       "98       0         couple having sex fat naked japanese girls\n",
       "99       0  #hump on that    #hump day #humpersð© @ edwa...\n",
       "100      0  personalised we... gbp 7.99 get here:  #shop #...\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Hate Speech.tsv\", sep= \"\\t\", index_col='id')\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f24b0fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop(\"label\",axis=1)\n",
    "y=df.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b61d88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f1339a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaningTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, text_column='tweet', download_nltk=True, return_tokens=True,join_tokens=True):\n",
    "        \"\"\"\n",
    "        A custom transformer for cleaning text data\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        text_column : str, default='tweet'\n",
    "            The name of the column containing text to clean\n",
    "        download_nltk : bool, default=True\n",
    "            Whether to download NLTK resources\n",
    "        return_tokens : bool, default=True\n",
    "            Whether to return tokenized text or just cleaned text\n",
    "        \"\"\"\n",
    "        self.text_column = text_column\n",
    "        self.download_nltk = download_nltk\n",
    "        self.return_tokens = return_tokens\n",
    "        self.join_tokens = join_tokens\n",
    "\n",
    "        # Download NLTK resources if needed\n",
    "        if self.download_nltk:\n",
    "            nltk.download('punkt', quiet=True)\n",
    "            nltk.download('stopwords', quiet=True)\n",
    "\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def fix_encoding(self, text):\n",
    "        \"\"\"Fix potential encoding issues\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "        try:\n",
    "            return text.encode('latin1').decode('utf-8')\n",
    "        except Exception:\n",
    "            return text\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean text by removing unwanted elements\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return \"\"\n",
    "\n",
    "        # Fix encoding\n",
    "        text = self.fix_encoding(text)\n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        # Remove @user\n",
    "        text = re.sub(r'@[\\w_]+', '', text)\n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "        # Remove hashtags (keep word)\n",
    "        text = re.sub(r'#', '', text)\n",
    "        # Convert emojis to text\n",
    "        text = emoji.demojize(text, language='en')\n",
    "        # Remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        # Remove punctuation\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        # Remove special characters except colons in emojis\n",
    "        text = re.sub(r'[^\\w\\s:]', '', text)\n",
    "        # Remove extra whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    def tokenize_and_remove_stopwords(self, text):\n",
    "        \"\"\"Tokenize text and remove stopwords\"\"\"\n",
    "        tokens = word_tokenize(text)\n",
    "        filtered_tokens = [word for word in tokens if word not in self.stop_words]\n",
    "        return filtered_tokens\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit method (does nothing but required by sklearn API)\"\"\"\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform the input data by cleaning the text\"\"\"\n",
    "        X_transformed = X.copy()\n",
    "\n",
    "        # Step 1: Fix encoding\n",
    "        X_transformed['clean_text'] = X_transformed[self.text_column].apply(self.fix_encoding)\n",
    "\n",
    "        # Step 2: Clean the text\n",
    "        X_transformed['clean_text'] = X_transformed['clean_text'].apply(self.clean_text)\n",
    "\n",
    "        # Step 3: Tokenize and remove stopwords if requested\n",
    "        if self.return_tokens:\n",
    "            X_transformed['tokens'] = X_transformed['clean_text'].apply(self.tokenize_and_remove_stopwords)\n",
    "        if self.join_tokens:\n",
    "            X_transformed['final'] = X_transformed['tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "        return X_transformed\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        \"\"\"Combine fit and transform methods\"\"\"\n",
    "        return self.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "049cfb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54be3699",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_cleaner = TextCleaningTransformer(text_column='tweet', download_nltk=True, return_tokens=True)\n",
    "\n",
    "# Apply fit_transform on x_train\n",
    "x_train_cleaned = text_cleaner.fit_transform(x_train)\n",
    "\n",
    "# Apply only transform on x_test (using the fitted transformer)\n",
    "x_test_cleaned = text_cleaner.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b13bcbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=x_train_cleaned[\"final\"]\n",
    "df_test=x_test_cleaned[\"final\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f02d4eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[122, 1288, 10, 962, 3146, 13165, 2107, 815, 815]\n",
      "[   0    0    0 ... 2107  815  815]\n",
      "1316\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = Tokenizer(num_words = 50000, split = ' ', lower = True, oov_token = 'UNK')\n",
    "tokenizer.fit_on_texts(df_train)\n",
    "train = tokenizer.texts_to_sequences(df_train)\n",
    "print(train[0])\n",
    "x = pad_sequences(train)\n",
    "print(x[0])\n",
    "print(x.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5659b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25228, 50)\n",
      "[  122  1288    10   962  3146 13165  2107   815   815     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_padded_sequences = pad_sequences(\n",
    "    train,\n",
    "    maxlen=50,\n",
    "    padding='post',\n",
    "    truncating='post'\n",
    ")\n",
    "\n",
    "print(train_padded_sequences.shape)\n",
    "print(train_padded_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24b65e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test= tokenizer.texts_to_sequences(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18884cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_padded_sequences = pad_sequences(test, maxlen=50, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52ddb1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_bi = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(50000, 16),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.15),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3666e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',     # you can also use 'val_accuracy'\n",
    "    patience=8,             # stop if no improvement for 5 epochs\n",
    "    restore_best_weights=True,  # restores best model (not just last)\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3491d4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "gru_bi.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f86695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=y_test.to_numpy()\n",
    "y_train=y_train.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13ed0112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 37ms/step - accuracy: 0.9320 - loss: 0.2483 - val_accuracy: 0.9573 - val_loss: 0.1288\n",
      "Epoch 2/30\n",
      "\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 37ms/step - accuracy: 0.9820 - loss: 0.0581 - val_accuracy: 0.9581 - val_loss: 0.1273\n",
      "Epoch 3/30\n",
      "\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.9938 - loss: 0.0215 - val_accuracy: 0.9474 - val_loss: 0.1927\n",
      "Epoch 4/30\n",
      "\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 37ms/step - accuracy: 0.9966 - loss: 0.0118 - val_accuracy: 0.9317 - val_loss: 0.2659\n",
      "Epoch 5/30\n",
      "\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.9985 - loss: 0.0066 - val_accuracy: 0.9475 - val_loss: 0.3102\n",
      "Epoch 6/30\n",
      "\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 41ms/step - accuracy: 0.9990 - loss: 0.0042 - val_accuracy: 0.9445 - val_loss: 0.2299\n",
      "Epoch 7/30\n",
      "\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - accuracy: 0.9991 - loss: 0.0037 - val_accuracy: 0.9491 - val_loss: 0.4134\n",
      "Epoch 8/30\n",
      "\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - accuracy: 0.9990 - loss: 0.0032 - val_accuracy: 0.9201 - val_loss: 0.3928\n",
      "Epoch 9/30\n",
      "\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.9994 - loss: 0.0023 - val_accuracy: 0.9513 - val_loss: 0.4525\n",
      "Epoch 10/30\n",
      "\u001b[1m789/789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 37ms/step - accuracy: 0.9993 - loss: 0.0015 - val_accuracy: 0.9344 - val_loss: 0.4524\n",
      "Epoch 10: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n"
     ]
    }
   ],
   "source": [
    "history = gru_bi.fit(train_padded_sequences, y_train, epochs=30, batch_size=32,\n",
    "                    validation_data=(test_padded_sequences, y_test),callbacks=[early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19353073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m198/198\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step\n"
     ]
    }
   ],
   "source": [
    "p = gru_bi.predict(test_padded_sequences,verbose=1)\n",
    "predicted = [int(round(x[0])) for x in p]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6af93056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8380695411460728\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(y_test, predicted,average=\"macro\")\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b213cce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "gru_bi.save(\"hate_speech_Gru_model.h5\")\n",
    "\n",
    "# Save tokenizer\n",
    "import pickle\n",
    "with open(\"tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b740b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step\n",
      "{'label': 'Hate Speech', 'probability': 0.6592000126838684}\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "{'label': 'Not Hate Speech', 'probability': 0.19470000267028809}\n"
     ]
    }
   ],
   "source": [
    "def predict_hate(text, threshold=0.2):\n",
    "    \"\"\"\n",
    "    Predict if the given text is hate speech or not.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "        threshold (float): Probability threshold for classification.\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains label and probability.\n",
    "    \"\"\"\n",
    "    # Convert to sequence and pad\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(sequence, maxlen=50, padding='post', truncating='post')\n",
    "\n",
    "    # Predict\n",
    "    prob = gru_bi.predict(padded)[0][0]\n",
    "    label = \"Hate Speech\" if prob >= threshold else \"Not Hate Speech\"\n",
    "\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"probability\": float(round(prob, 4))\n",
    "    }\n",
    "\n",
    "# 💡 Example usage:\n",
    "print(predict_hate(\"You people are nothing but a plague to this country. Always whining and ruining everything. Go back to where you came from!\"))\n",
    "print(predict_hate(\"Hope you have a great day!\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a591db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
